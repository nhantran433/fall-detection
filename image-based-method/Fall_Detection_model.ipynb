{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU3DEesMPRMl"
      },
      "source": [
        "#Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mumNWjunCem5"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        " return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmN6d1AECPIU",
        "outputId": "13835e79-59e9-4469-885a-e8e6ddfecf57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch torchvision scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lmGGmVdmdc2"
      },
      "source": [
        "#Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkgecdmGwDwD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn. ensemble import RandomForestClassifier\n",
        "import pickle\n",
        "from sklearn.utils import class_weight\n",
        "from shutil import rmtree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq2UdTCgvrYM"
      },
      "source": [
        "#Visualization the image-predicted_label and grouth_truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKT2vJgQPXbr"
      },
      "outputs": [],
      "source": [
        "class_names = ['Fall', 'Not Fall']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf7USqiL7eBH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def visualize_images(images, y_pred, y_true, n=5):\n",
        "    # Randomly select n samples from the dataset.\n",
        "    indices = random.sample(range(len(images)), n)\n",
        "\n",
        "    # Number of columns and number of rows\n",
        "    cols = 4\n",
        "    rows = (n + cols - 1) // cols\n",
        "\n",
        "    # Create a figure with rows rows and cols columns\n",
        "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(15, rows * 3))\n",
        "\n",
        "    # If there are fewer images than the number of columns, special handling is needed.\n",
        "    axes = axes.flatten()  # Convert the axes array to 1D for easier iteration.\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Display the label in green.\n",
        "        ax.text(0.5, 1.15, f'True: {class_names[int(y_true[idx])]}', color='green', fontsize=12, ha='center', transform=ax.transAxes)\n",
        "\n",
        "        # Display the predicted results in red if they differ from label.\n",
        "        pred_color = 'red' if y_pred[idx] != y_true[idx] else 'green'\n",
        "        ax.text(0.5, 1.05, f'Predicted: {class_names[int(y_pred[idx])]}', color=pred_color, fontsize=12, ha='center', transform=ax.transAxes)\n",
        "\n",
        "        # Display image\n",
        "        ax.imshow(cv2.cvtColor(images[idx],cv2.COLOR_BGR2RGB))\n",
        "        ax.axis('off')  # Turn off axis display\n",
        "\n",
        "    # Hide unused axes if the number of images is not enough to fill the entire figure.\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1po1ssamgO9"
      },
      "source": [
        "#Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUc9-8SlPaE9"
      },
      "outputs": [],
      "source": [
        "main_dir = './dataset'\n",
        "\n",
        "imges_dir = os.path.join(main_dir, 'images')\n",
        "labels_dir = os.path.join(main_dir, 'labels')\n",
        "\n",
        "name_img_files = os.listdir(imges_dir)\n",
        "name_label_files = os.listdir(labels_dir)\n",
        "\n",
        "name_img_files.sort()\n",
        "name_label_files.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIrZkrcTu6jc"
      },
      "outputs": [],
      "source": [
        "def get_most_common_size_with_threshold(sizes,threshold = 0.25):\n",
        "  # Find the most common size.\n",
        "  size_counts = Counter(sizes)\n",
        "  total_images = len(sizes)\n",
        "  most_common_size = max(size_counts, key=size_counts.get)\n",
        "  if size_counts[most_common_size] / total_images < threshold:\n",
        "    most_common_size = None\n",
        "  return most_common_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP5C_-KduERF"
      },
      "outputs": [],
      "source": [
        "def get_aspect_ratio(width, height):\n",
        "    return width / height\n",
        "\n",
        "def get_average_size_with_aspect_ratio(widths,heights):\n",
        "  average_width = int(np.mean(widths))\n",
        "  average_height = int(np.mean(heights))\n",
        "\n",
        "  # Find the most common aspect ratio.\n",
        "  aspect_ratios = [get_aspect_ratio(w, h) for w, h in zip(widths, heights)]\n",
        "  most_common_aspect_ratio = Counter(aspect_ratios).most_common(1)[0][0]\n",
        "\n",
        "  # Compute the new dimensions based on the most common aspect ratio.\n",
        "  if get_aspect_ratio(average_width, average_height) != most_common_aspect_ratio:\n",
        "      average_height = int(average_width / most_common_aspect_ratio)\n",
        "\n",
        "  return (average_width, average_height)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_xqys6zvZxH"
      },
      "outputs": [],
      "source": [
        "def get_suitable_size(sizes,widths,heights,threshold = 0.25):\n",
        "  # Find the most common size.\n",
        "  most_common_size = get_most_common_size_with_threshold(sizes,threshold)\n",
        "  if most_common_size is None:\n",
        "    most_common_size = get_average_size_with_aspect_ratio(widths,heights)\n",
        "  return most_common_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxQ4K6Qi8h9P"
      },
      "outputs": [],
      "source": [
        "def custom_ceil(value):\n",
        "    fractional_part = value - np.floor(value)\n",
        "    if fractional_part > 0.5:\n",
        "        return np.ceil(value)\n",
        "    else:\n",
        "        return np.floor(value)\n",
        "\n",
        "def resize_to_nearest_power_of_two(image_size, min_dimension=7):\n",
        "    width, height = image_size\n",
        "\n",
        "    new_width = min_dimension * 2**custom_ceil(np.log2(width / min_dimension))\n",
        "    new_height = min_dimension * 2**custom_ceil(np.log2(height / min_dimension))\n",
        "\n",
        "    new_width = int(new_width)\n",
        "    new_height = int(new_height)\n",
        "\n",
        "    return (new_width,new_height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYyqW3LNx0Yq"
      },
      "outputs": [],
      "source": [
        "def resize_image(images, size):\n",
        "    resized_images = []\n",
        "    for image in images:\n",
        "        resized_images.append(cv2.resize(image, size))\n",
        "    return resized_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNpSfUBQ5zdA"
      },
      "source": [
        "Sử dụng Image_dataset_from_directory tiết kiệm dung lượng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m74KNg8u53u-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from shutil import rmtree\n",
        "\n",
        "def save_image_to_directory(img, label, output_dir, img_name):\n",
        "    global class_names\n",
        "    # Create the label directory if it doesn't exist\n",
        "    label_dir = os.path.join(output_dir, str(class_names[label]))\n",
        "    os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "    # Save the image in the corresponding label directory\n",
        "    img_path = os.path.join(label_dir, img_name)\n",
        "    cv2.imwrite(img_path, img)\n",
        "\n",
        "def create_data_label(images_dir, labels_dir, images_files, labels_files, folder='train', threshold=0.25, is_fall_value=0, output_dir='output'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    # if os.path.exists(output_dir):\n",
        "    #     rmtree(output_dir)\n",
        "\n",
        "    face_img_sizes = []\n",
        "    face_img_heights = []\n",
        "    face_img_widths = []\n",
        "    num_faces = 0\n",
        "\n",
        "    for count, images_file in enumerate(images_files):\n",
        "        if images_file.endswith(('jpg', 'jpeg', 'png', 'gif', 'bmp')):\n",
        "            image_path = os.path.join(images_dir, folder, images_file) if folder is not None else os.path.join(images_dir, images_file)\n",
        "            img = cv2.imread(image_path)\n",
        "            if img is not None:\n",
        "                # Read the corresponding label file\n",
        "                label_path = os.path.join(labels_dir, folder, labels_files[count]) if folder is not None else os.path.join(labels_dir, labels_files[count])\n",
        "                with open(label_path, 'r') as file:\n",
        "                    r = file.readlines()\n",
        "\n",
        "                bounding_boxes = []\n",
        "                for values in r:\n",
        "                    value = values.split()\n",
        "                    bounding_boxes.append([int(value[0]), float(value[1]), float(value[2]), float(value[3]), float(value[4])])\n",
        "\n",
        "                for box in bounding_boxes:\n",
        "                    image_height, image_width, channels = img.shape\n",
        "\n",
        "                    # Get bounding box of the element (person)\n",
        "                    x_min = int(box[1] * image_width)\n",
        "                    y_min = int(box[2] * image_height)\n",
        "                    box_width = int(box[3] * image_width)\n",
        "                    box_height = int(box[4] * image_height)\n",
        "\n",
        "                    # Calculate crop limits (ensure not to exceed image boundaries).\n",
        "                    x_start = max(0, x_min - box_width // 2)\n",
        "                    y_start = max(0, y_min - box_height // 2)\n",
        "                    x_end = min(image_width, x_min + box_width // 2)\n",
        "                    y_end = min(image_height, y_min + box_height // 2)\n",
        "\n",
        "                    # Crop the image to get the element\n",
        "                    cropped_img = img[y_start:y_end, x_start:x_end]\n",
        "                    if cropped_img is not None and cropped_img.shape[0] > 0 and cropped_img.shape[1] > 0:\n",
        "                        label = int(box[0] != is_fall_value)\n",
        "                        # Save the cropped image into the appropriate folder\n",
        "                        save_image_to_directory(cropped_img, label, output_dir, f\"{images_file}_cropped_{count}.jpg\")\n",
        "\n",
        "                        # Track face sizes for determining suitable size\n",
        "                        height_face, width_face, _ = cropped_img.shape\n",
        "                        face_img_sizes.append((width_face, height_face))\n",
        "                        face_img_widths.append(width_face)\n",
        "                        face_img_heights.append(height_face)\n",
        "                        num_faces += 1\n",
        "\n",
        "    if num_faces == 0:\n",
        "        raise ValueError(\"No images found in the specified directory or the number of labels and images isn't the same.\")\n",
        "    return get_suitable_size(face_img_sizes, face_img_widths, face_img_heights, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L-BrtjpQAgn"
      },
      "outputs": [],
      "source": [
        "suitable_size = create_data_label(imges_dir,labels_dir,name_img_files,name_label_files,folder = None,is_fall_value = -1,output_dir = './dataset_tensorflow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd2G7eJsLVQg",
        "outputId": "3e1c7c18-6d26-4ebf-bb1d-0dc0b7ffb060"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(224, 448)"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resize_to_nearest_power_of_two((214, 428),7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDBSMTIdt0Cq"
      },
      "source": [
        "#Evaluation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eEFC-8ctzVm"
      },
      "outputs": [],
      "source": [
        "def evaluation_model(y_true,y_pred):\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "\n",
        "  print(\"Classification Report:\")\n",
        "  print(classification_report(y_true, y_pred))\n",
        "\n",
        "  conf_mat = confusion_matrix(y_true, y_pred)\n",
        "  displ = ConfusionMatrixDisplay(confusion_matrix=conf_mat)\n",
        "  displ.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzgzMO6gsd_-"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2FvLpMGk7rq"
      },
      "source": [
        "##Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm9pznxFot_G"
      },
      "source": [
        "###Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpefOFQ0lwbl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import layers as L\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization,Rescaling\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, BinaryCrossentropy,categorical_crossentropy,categorical_accuracy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.src.applications import imagenet_utils\n",
        "from tensorflow.keras.applications import VGG16,MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzfha9-tw4ja"
      },
      "outputs": [],
      "source": [
        "def compute_class_weights(y_train):\n",
        "    class_weights = class_weight.compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "    return dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Kc-g6nLlM2"
      },
      "source": [
        "Dùng image_dataset_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AROBaRLfLp3n"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "image_size = (448, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vv41Q3JLrZj",
        "outputId": "671cfc10-b765-410c-dfd0-e27c3a960706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 48086 files belonging to 2 classes.\n",
            "Using 38469 files for training.\n",
            "Found 48086 files belonging to 2 classes.\n",
            "Using 9617 files for validation.\n"
          ]
        }
      ],
      "source": [
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/dataset\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    label_mode='binary' ,\n",
        "    #labels = n_classes,\n",
        "    interpolation = \"mitchellcubic\",\n",
        "    seed=42,\n",
        "    color_mode=\"rgb\",\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/dataset\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    label_mode='binary' ,\n",
        "    #labels = n_classes,\n",
        "    interpolation = \"mitchellcubic\",\n",
        "    seed=42,\n",
        "    color_mode=\"rgb\",\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hINLNtmL7Ay"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "# Lấy nhãn từ dataset\n",
        "labels = np.concatenate([y for x, y in train_ds], axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qKSGAu1RQMo"
      },
      "outputs": [],
      "source": [
        "labels = labels.ravel()\n",
        "\n",
        "# Tính toán class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwts80nwRP-r"
      },
      "source": [
        "####MobileNet v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4me6QEBlRR7Z"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Rescaling, GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "def create_mobilenet_2_model(input_shape, num_classes):\n",
        "    # Load the pre-trained MobileNetV2 model\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Freeze the layers of the base model\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Define the input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Add the Rescaling layer\n",
        "    x = Rescaling(1. / 255)(inputs)\n",
        "\n",
        "    # Add the base model\n",
        "    x = base_model(x)\n",
        "\n",
        "    # Add custom classification layers\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(512, activation='mish')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(512, activation='mish')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    if num_classes == 2:\n",
        "        x = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
        "    else:\n",
        "        x = Dense(num_classes, activation='softmax')(x)  # Multi-class classification\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "TzjgoIpMUYgf",
        "outputId": "3b3211e1-fa93-452a-c2e3-619a9a616a8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-32-0d0921450882>:7: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_36\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_36\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ rescaling_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mobilenetv2_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_14 (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m448\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ rescaling_4 (\u001b[38;5;33mRescaling\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m448\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ mobilenetv2_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)         │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m655,872\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m262,656\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m513\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,177,025</span> (12.12 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,177,025\u001b[0m (12.12 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">919,041</span> (3.51 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m919,041\u001b[0m (3.51 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_mobilenet2 = create_mobilenet_2_model(input_shape =  list(image_size)+[3], num_classes = len(class_names))\n",
        "model_mobilenet2.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1) if len(class_names) == 2 else tf.keras.losses.categorical_crossentropy(from_logits=True, label_smoothing=0.1),\n",
        "    optimizer=tf.keras.optimizers.AdamW(),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        ")\n",
        "model_mobilenet2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoPValbqUfZt"
      },
      "outputs": [],
      "source": [
        "model_path=\"/content/best_check_point/{}_weights.best.keras\".format('mobilenetv2')\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_binary_accuracy', factor=0.75, patience=3, min_delta=0.001,\n",
        "                          mode='max', min_lr=1e-6, verbose=1),\n",
        "    ModelCheckpoint(model_path, monitor='val_binary_accuracy', verbose=1,\n",
        "                             save_best_only=True, mode='max', save_weights_only = False),\n",
        "    EarlyStopping(monitor='val_binary_accuracy',patience=6,verbose=1,mode='max',restore_best_weights = True)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_mobilenet2.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1) if len(class_names) == 2 else tf.keras.losses.categorical_crossentropy(from_logits=True, label_smoothing=0.1),\n",
        "    optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-4),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur0oBnw9UjO-"
      },
      "outputs": [],
      "source": [
        "history_mobilenet2 = model_mobilenet2.fit(train_ds, validation_data=val_ds, verbose=1, epochs=5,class_weight =class_weights_dict,callbacks = callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUma9ONCeYpS"
      },
      "outputs": [],
      "source": [
        "model_mobilenet2.save('model_mobilenet2.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7iUIuG7ZsdT"
      },
      "outputs": [],
      "source": [
        "y_pred_mobilenet2 = []\n",
        "y_true = []\n",
        "\n",
        "for images, labels in val_ds:\n",
        "    preds = model_mobilenet2.predict(images, verbose=0)\n",
        "    preds = tf.squeeze(preds)\n",
        "\n",
        "    y_pred_mobilenet2.extend([1 if x >= 0.5 else 0 for x in preds])\n",
        "    y_true.extend(labels.numpy())\n",
        "\n",
        "y_pred_mobilenet2 = np.array(y_pred_mobilenet2)\n",
        "y_true = np.array(y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "XakJQ1oflZOz",
        "outputId": "115fb365-1418-4625-e85d-d665863be522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9653738171987106\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.96      0.96      4614\n",
            "         1.0       0.96      0.97      0.97      5003\n",
            "\n",
            "    accuracy                           0.97      9617\n",
            "   macro avg       0.97      0.96      0.97      9617\n",
            "weighted avg       0.97      0.97      0.97      9617\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "evaluation_model(y_true, y_pred_mobilenet2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
